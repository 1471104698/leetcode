# 简单总结四



## 1、HTTP 状态码 和 报文格式

```
1xx 表示提示信息
【100 Continue】用于 post 请求两次传输，浏览器第一次将 post 请求头发送给服务器，服务器根据 post 请求头部的信息（content-length、Authenticate 等字段）进行验证和判断是否能够进行处理，如果可以，那么响应 100 让浏览器发送请求体

2xx 表示请求成功
【200 OK】表示服务器请求处理成功，可能会存在数据，数据存储在 响应体中
【201 Created】表示需要创建的资源已经创建完毕，资源的 URL 在响应头的 Location 字段中
【202 Accepted】表示服务器接收到请求，但是还没有处理，可能后面处理，也可能不会处理
【204 Not Content】服务器请求处理成功，没有数据返回给浏览器
【206 xxx】用于断点续传，表示还没有数据发送完成

3xx 跟资源相关
【301 Moved Permanently】永久重定向，表示资源的位置永久发生改变，在响应头的 Location 字段存储了新的 URL，浏览器会 cache 该 URL
【302 Found / Moved Temporary】临时重定向，表示资源的位置暂时发生改变，在响应头的 Location 字段存储了新的 URL，浏览器不会 cache 该 URL，此次访问新的 URL，下次访问还是旧的 URL
【304 not modified】资源没有更新，浏览器的 cache 数据仍然可以使用（在输入 url 访问的全过程中会提到）

4xx 客户端请求错误
【401 Unauthorized】用户未认证，需要先认证
【403 Forbidden】用户已经认证，但是没有权限访问请求的资源
【404 Not Found】用户请求的资源没有找到（不存在）

可以看出，它们常见的状态码都不存在 x03 这个状态码

5xx 服务器错误
【502】



HTTP 请求报文的格式（响应报文类似）：
1）请求行：请求的 url 和 http 版本号，请求的方式
2）请求头
3）空行：用来分割请求头 和 请求体的
4）请求体


HTTP 请求方式 
1）GET：用于请求静态资源（图片、html），请求的参数存储在 url 上，请求体中不会存储数据
2）POST：请求服务器创建资源，需要创建的资源存储在请求体中
3）PUT：用于修改指定位置的资源
4）DELETE：用于删除指定位置的资源
5）HEAD：该请求跟 GET 类似，区别在于响应体不存在任何数据，主要是用来资源检查，比如检查缓存是否过期了

GET 和 POST 的区别：
1）GET 是用来请求静态资源，请求参数存储在 URL 上，HTTP 协议没有限制 URL 的大小，但是浏览器会进行限制；POST 是用来创建资源，请求参数存储在 请求体中，HTTP 协议没有限制请求体的大小，但是服务器（tomcat）会进行限制
2）GET 是幂等的，无论多少次请求都不会改变服务器资源，POST 是非幂等的，多次请求就会创建多个资源
3）GET 只能分为一次发送（当然 HTTP 协议没有规定），POST 协议有时候能够分为两次发送（即我们上面讲的 100 状态码）

为什么 POST 要分为两次发送？
当 POST 的请求体参数过大时，有时候服务器无法进行处理，那么将整个 POST 发送过去，服务器又不能进行处理，那么显然这次请求是无意义的，但是请求体的数据又会占据网络带宽。所以为了避免这种情况的发生，浏览器会先发送请求头给服务器，服务器经过验证后如果自己能够处理，那么再响应 100 状态码让浏览器发送请求体。
这么做的好处是在一定程度上能够减少网络带宽的占用
缺点是需要分为两次发送，需要占用更多的时间
```



## 2、HTTP 各个版本的演变逻辑

```java
1、HTTP 0.9
它只有 GET 请求，同时只能简单的处理 html 字符串
（可以说这时候只有一个简单的请求行和请求体，请求行没有多余的 HTTP 版本 和 HTTP 请求方式，同时也没有设置状态码和请求头）

2、HTTP 1.0
1）添加了 POST、PUT 等请求方式
2）能够处理 图片、音频 等文件
3）增加了请求头/响应头、状态码等

HTTP 1.0 的缺点：
使用短连接，每一个请求都需要重新建立一个 TCP 连接，效率较低


3、HTTP 1.1
1）长连接：添加了长连接的连接方式，默认使用长连接，当一个 HTTP 请求和响应完成后，不会立马断开连接。 
2）管道：添加管道，一个 TCP 连接能够同时发送多个 HTTP 请求（管道是基于长连接的）
3）断点续传：跟 206 状态码配合使用

HTTP 1.1 的缺点：
随着基于长连接的管道，存在 HTTP 队头阻塞的问题（短连接没有这个问题，长连接才有）：能够同时发送多个 HTTP 请求，但是必须按照顺序进行响应
在一条 TCP 连接上，先后发送了 请求 1 和 请求 2，服务器即使先处理好请求 2，但是也不能直接响应请求 2，只能等请求 1 处理并响应完才能去响应请求 2，因为这个 TCP 连接没有任何措施去标识响应对应的是哪个请求的，如果先将 响应 2 发送出去，那么可能会被当成是 请求 1 的响应



4、HTTP 2.0
1）代替文本格式，使用新的二进制格式
2）多路复用
3）头部压缩
4）服务推送

二进制格式：
HTTP 2.0 的报文全部修改为二进制形式的，而不再使用 HTTP 1.x 的文本形式
使用二进制格式主要是应该为了多路复用

多路复用：
HTTP 2.0 使用多路复用的方式来解决 HTTP 1.1 的 HTTP 队头阻塞的问题：
	它将同个域名上的同个用户的所有请求共用一条 TCP 连接，它定义了 3 个新的概念：流（stream）、消息（message）、帧（frame），一个完整的 HTTP 请求/响应 的过程为 stream，一个 HTTP 请求 或者 响应为 message，每个消息会被分为多个 frame 进行传输，而在每个 frame 上会存在一个 stream id 标识该 frame 属于哪个 stream。 这么做的好处在于，它相比 HTTP 1.1 来说实现了一种机制，能够标识响应是属于哪个请求的，这样就不需要按照 请求的顺序来进行响应了，解决了 HTTP 队头阻塞的问题。
	多路复用要求同一个 stream 的 frame 必须按序传输，即 FIFO
这种多路复用存在另一个问题：TCP 队头阻塞：
	由于每个 HTTP 请求/响应 都是使用的一个 TCP 连接，意味着它们的数据都是共用同一个 TCP 发送缓冲区的，这样一旦存在某个 stream 的某个 frame 丢失的话，那么就会导致滑动窗口无法右移，导致所有 stream 的数据在可发送窗口为 0 并且在丢包重传完成之前都无法发送出去，出现 TCP 队头阻塞。当出现这种情况时，HTTP 2.0 的效率比 HTTP 1.1 还差，因为 HTTP 1.1 可以通过多开几个 TCP 连接来尽可能避免这种问题。
	
头部压缩：
在 HTTP 报文中，大部分的字段都是重复的，并且基本都不会发生改变，因此 HTTP 2.0 约定发送方和接收方各自维护一份相同的静态字典和动态字段。
静态字典用来存储 method:GET、set-cookie 这种基本不会发生改变的字段，在字典中它们都各自对应一个索引编号，在 HTTP 报文中直接使用这个编号来代替这个字段，接收方接收到后根据编号比对自己的静态字典来获取对应的字段值。这样可以节省很多的内存
动态字段跟静态字典差不多，主要是维护一些动态变化的数据，比如请求的 url、cookie 值之类的

服务推送:
在 HTTP 1.x 中，浏览器请求一个 html 页面，服务器只会返回 html 页面，浏览器解析后会发起多个请求来获取 html 页面中需要的 js、css、图片等，效率来说相对较低
在 HTTP 2.0 中，服务器会主动推送资源，当浏览器请求一个 html 页面时，服务器在响应 html 时认为浏览器还可能需要对应的 js、css、图片等资源，因此会在响应中顺便带上这些资源，减少浏览器的请求


HTTP 2.0 的缺点：
1）使用的多路复用存在 TCP 阻塞问题，一旦丢包重传那么会阻塞其他的 stream，效率比 HTTP 1.1 还低
2）HTTP 2.0 协议没有要求使用 SSL 协议，但是主流浏览器都强制要求 HTTP 2.0 使用 SSL 协议，那么就是需要 TCP 3 次握手 + SSL 四次握手，如果把 TCP 第三次握手和 SSL 第一次握手合并，那么也需要 6 次握手，那么单单为了连接就需要 3 RTT 的时间（物理距离越大，那么 RTT 越长）
    

5、HTTP 3.0

HTTP 3.0 的特性：
1）使用 UDP + DH 算法 来代替 TCP + SSL，实现 1 RTT 连接
2）使用 HTTP 2.0 的多路复用
3）应用层面实现 TCP 的滑动窗口机制、超时重传机制、流量控制机制、拥塞控制机制
4）使用 Packet number 实现 UDP 层面的序列号机制
5）使用 前向纠错 减少丢包重传的几率

HTTP 3.0 目前讲的是 QUIC 协议，它使用 UDP + DH 算法 来代替 TCP + SSL，实现 1 RTT 连接
使用 UDP 的目的是为了减少 TCP 握手、挥手带来的时间消耗以及 TCP 队头阻塞问题
同时为了保证可靠性传输，它在应用层面自己实现了类似 TCP 的可靠性算法：滑动窗口、超时重传、流量控制、拥塞控制（慢启动、拥塞避免、快重传、快恢复），这种在应用层面实现的不依赖于操作系统和内核，可以根据用户的网络情况制定不同的拥塞控制算法，更加的有效

虽然 HTTP 3.0 使用的是 UDP，但是它也实现了多路复用、滑动窗口和丢包重传，那么为什么不会出现 TCP/UDP 队头阻塞 这种问题？
我们知道 HTTP 2.0 的 TCP 队头阻塞是 滑动窗口 导致的问题
而 HTTP 3.0 虽然使用了滑动窗口，但是它本身也对这种机制做了一些改造，它定义了一个 Packet number，每个包都有一个 Packet number，这个 Packet number 可以当作是 UDP 层面的序列号机制
并且它规定了丢包重传的新包的 Packet number 要比丢失的旧包的 Packet number 要大，即丢包重传的包当作一个新的包来传输，比如 Packet N 丢失了，那么它重传的包应该是 Packet N + M，而不再是 Packet N。
这意味着即使发生丢包，滑动窗口也不会阻塞在丢包的位置，而会继续右移。

将丢失的包当作一个新的包不会导致同一个 stream 上的 Packet 的数据顺序发生改变吗?
在 HTTP 2.0 中，为什么要求同一个 stream 中需要按序传输？
这里我们需要先讲下 HTTP 和 TCP 之间的关系： TCP 保证的是将 HTTP 顺序交给它的数据进行编号，这个编号就叫做序列号，然后在 TCP 接收方根据这个序列号进行拼接，保证了顺序，因此滑动窗口当丢包重传时，需要按照原来的序列号进行重传，避免乱序，因此会才导致丢包重传时窗口阻塞。
这意味着 HTTP 按照顺序发送 1 2 4 3 的数据包给 TCP，TCP 本身是不知道这些数据包是乱序的，在它眼里，1 2 4 3 就是顺序，它的责任就是按照 1 2 4 3 的顺序发送，并且按照 1 2 4 3 的顺序进行拼接。
因此在 HTTP 层面必须保证数据是有序的，否则 TCP 会乱序发送，这也正是 HTTP 2.0 中 stream 的 frame 必须按序发送的原因，正是因为它没有跟 TCP 的序列号机制一样的机制，所以它才需要按序发送，所以它才会导致窗口阻塞
而在 HTTP 3.0，它给每个 Packet 标识了所在 stream 的 offset，这个 offset 就相当于 HTTP 层面的序列号机制，即使乱序发送，那么 HTTP 层面的接收方也可以通过这个 offset 正确拼接，因此在丢包重传时可以将包当作心的包发送，因为我们不需要在 TCP 层面保证顺序，而是在 HTTP 层面保证顺序。


前向纠错的作用：
比如一段数据被切成 10 个 Packet 包，对 10 个包进行异或运算，运算结果会作为 FEC 包连同 10 个包数据一起传输过去，如果在传输过程中除了 FEC 包外存在其他的一个包丢失了，那么可以根据 FEC 包 和 其他 9 个包来推算出丢失包的数据，从而减少丢包重传的几率
虽然说多发了一个包，但是这个代价比丢包重传（重传需要时间，并且后续的数据也会进行阻塞）要小得多。
异或原理：a^b = c，a^c = b，
因此 p1^p2^p3^p4^p5^p6^p7^p8^p9^p10 = FEC
那么如果丢失了一个包，可以通过 p1^p2^p3^p4^p5^p6^p7^p8^p9^FEC = p10 来计算出丢失的包的数据
但是仅仅只能用在丢失一个包的情况，如果丢失了 两个及两个以上的包，那么就只能重传了
```





## 3、HTTPS

```java
HTTPS 是在 应用层 和 传输层之间加的一层 SSL/TCL 协议，可以说是在表示层的协议

由于单纯的使用 AES 对称加密 和 RSA 非对称加密存在中间人攻击问题，所以需要出现了 CA 证书

CA 证书是 CA 机构颁发的，CA 证书的生成过程：
1）服务器生成一对非对称密钥，然后将公钥和服务器域名信息、持有人 等信息发送给 CA 机构
2）CA 机构自己有一对非对称密钥，它使用一个散列算法对 服务器信息进行 hash 计算生成 信息摘要，然后使用自己的 CA 私钥对信息摘要进行加密
3）CA 机构将 服务器信息明文、散列算法、加密后的信息摘要 封装起来作为一个 CA 证书，颁发给服务器

客户端如何验证 CA 证书？
CA 公钥是内置在计算机上的，相当于每个计算机都有世界公认的 CA 机构的 CA 公钥，当客户端拿到服务器的 CA 证书后，它通过 CA 证书上的 CA 机构从自己的计算机中获取对应的 CA 公钥，然后解密 被加密后的信息摘要，然后将服务器信息明文使用 CA 证书上的散列算法进行计算，然后跟解密后的信息摘要进行比对，如果一致，那么表示服务器可信任。
    
CA 证书如何防止篡改？
CA 证书可以被拦截，但是不能被篡改：
1）因为中间人拦截了 CA 证书后，如果它修改了服务器信息明文，那么客户端验证的时候发现两个信息摘要不一致，那么不会信任；
2）如果它修改了服务器明文，同时使用自己的 CA 公钥解密信息摘要，然后替换掉信息摘要，但是它没有 CA 私钥，所以无法正确加密，这样客户端解密失败，同样无法信任

SSL 四次握手过程：
1）第一次握手：客户端携带一个随机数（第一个随机数）和自己支持的对称加密算法
2）第二次握手：服务器收到信息后，保存随机数，然后自己初始化一个随机数（第二个），将随机数、CA 证书 和 选定的加密算法发送给客户端
3）第三次握手：客户端收到 CA 证书，保存随机数，然后再初始化一个随机数（第三个），利用服务器选定的加密算法和三个随机数生成一个对称密钥，然后使用对称密钥加密一段握手信息，然后使用 CA 证书上的 服务器公钥加密握手信息 和 第三个随机数，发送给服务器
4）第四次握手：服务器收到后，使用 私钥进行解密，然后使用三个随机数 和 加密算法生成对称密钥，然后使用对称密钥解密 握手信息，解密成功表示跟客户端生成对称密钥一样，然后使用这个对称密钥加密一段握手信息，发送给客户端
客户端收到后解密成功，表示服务器跟自己的对称密钥一致，那么可以开始通信。


在 TCP 握手的第三次握手中，客户端可以同时发起 SSL 第一次握手，相当于 TCP + SSL 压缩为 6 次握手，减少了一次
```





## 4、TCP 三次握手 / 四次挥手

```
1、TCP 三次握手
第一次握手：发送方创建 TCP 报文，初始化一个随机数 seq1 作为序列号，作为将 SYN 标志位置 1，发送给接收方，此时处于 SYN_SENT 状态
第二次握手：接收方收到后，创建一个 TCP 报文，初始化一个随机数 seq2 作为序列化，然后将 确认号设置为 seq1 + 1，然后将 SYN、ACK 标志位置 1，然后发送给发送方，此时处于 SYN_RECV 状态
第三次握手：发送方收到后，创建一个 TCP 报文，将序列号设置为 seq1 + 1，然后确认号设置为 seq2 + 1，将 SYN、ACK 标志位置 1，发送给接收方，此时处于 ELxxx（建立）状态
接收方收到后，处于 ELxxx（建立）状态

为什么不能是两次握手？
有 3 个原因：
    1）确认双方的发送和接收能力都正常
    2）避免历史连接：比如发送方第一次发送了 SYN，结果由于网络拥堵，发送方再次发送一个 SYN，此时第一个 SYN 到达接收方，接收方回应 SYN + ACK，发送方再回应 ACK，而此时第二次发送的 SYN 也到达了，如果只有两次握手，那么接收方自己并不知道它是历史连接，将它放入到 SYN 队列中，然后回应 ACK，这样就会导致重复连接。而如果是三次握手，在第三次握手中发送方发现这是历史连接，可以发出 RST 中止这个历史连接
    3）确保双方都同步了初始序列号：初始序列号在后续的传输中极其重要，如果只有两次握手，那么接收方并不清楚发送方是否已经同步了自己的初始序列号

为什么不能是四次握手?
    如果是四次握手，那么过程如下：
        第一次握手：发送 SYN
        第二次握手：发送 ACK
        第三次握手：发送 SYN + ACK
        第四次握手：发送 ACK
    跟四次挥手不一样，第二次握手和第三次握手之间不需要什么处理，所以可以直接将第二次握手和第三次握手进行合并

为什么要使用随机初始序列号？
	为了安全性，随机初始序列号是告知对方我们要发送的数据是在这个偏移量上的，比如随机初始序列号为 1000，然后我发送了 1 号字节，那么发送出去的数据的序列号为 1001，减去初始序列号后变成了 1，即发送的数据是整个数据的 1 号字节数据
	如果 TCP 的数据发送都从 0 号开始，那么黑客可以很简单的猜测到 报文的序列号和确认号，从而可以伪造 TCP 报文，比如发送一个 RSET 报文，接收方发现序列号和确认号是对的，所以直接中止了连接。所以使用随机序列号的作用是为了减少黑客攻击成功的几率。因此三次握手同步初始序列号是很重要的
	

2、TCP 四次挥手
第一次挥手：发送方创建一个 TCP 报文（后面都忽略掉这个步骤），将 FIN、ACK 标志位置 1，发送给接收方，此时处于 FIN_WAIT1 状态
第二次挥手：接收方收到，先发送一个 ACK 报文，告知发送方自己收到终止请求，不过此时自己可能有数据还需要处理或者发送，所以此时接收方不会立马发送 FIN 报文，此时处于 CLOSE_WAIT 状态
发送方收到 ACK 报文后，处于 FIN_WAIT2 状态
第三次挥手：接收方处理完数据后，发送 FIN + ACK 报文，此时处于 LAST_ACK 状态
第四次挥手：发送方收到后，发送 ACK 报文，此时处于 TIME_WAIT 状态，这里一般是 2MSL
接受方收到后，进入 CLOSED 状态，发送方 TIME_WAIT 过后，自动进入 CLOSED 状态


为什么需要 TIME_WAIT?
[为什么 TCP 协议有 TIME_WAIT 状态](https://draveness.me/whys-the-design-tcp-time-wait/)
1）为了让被动关闭方能够正常的收到 FIN 的 ACK 然后正常关闭：如果客户端返回 ACK 后就直接进入 CLOSED 状态，如果这个 ACK 在网络中丢失了，那么服务端没有收到，这段时间会一直处于 LAST_ACK 状态，如果这段时间客户端重用了刚刚的四元组发起 SYN 请求，那么服务器会返回一个 RST 终止连接
2）让旧的数据包在网络中消失，不会出现在新的连接上：如果发送完 ACK 直接进入 CLOSED，客户端重用连接，那么在网络中原本属于旧的连接的数据包可能会到达这个新的连接上，导致混乱。

TIME_WAIT 为什么是 2MSL？
1）报文的最大有效时间是 MSL，一旦超过这个时间，那么网络中的报文就无效了，等待 2MSL 必定能使属于此次连接的数据包无效，而不会应用到下一次新的连接中
2）当发送方第四次握手发送 ACK 后，这个报文能够到达接收方的临界时间为 MSL，而接收方在等待了一段时间后，重新发送 FIN + ACK包，这个临界时间也是 MSL，就是当作 ACK 刚刚到达接收方前，接收方又重发了 FIN 这种极端情况，这样的话就又最大需要等待 MSL，如果在这个 2MSL 时间内没有收到 FIN，那么就可以关闭了，因为即使有旧的 FIN + ACK 也已经经过 MSL 无效了

```



## 5、TCP 可靠性实现

```
1、序列号和确认号机制


2、超时重传、快速重传


3、滑动窗口

滑动窗口出现的目的：
1）提高发送的效率，不再是发送一个包就等待一个 ACK，而是采用累计确认的方式
2）为了实现流量控制

在传统的 TCP 中，由于 TCP 要保证可靠传输，同时要保证有序性，所以它提供可靠性的是发一个包就等待一个包的 ACK，并且按照顺序发送包，发送包1，需要等待包1 的 ACK，才能继续发送包2，这样就保证了可靠性和有序性。
这个方法的问题在于：效率太低，这明显是很有问题的。

因此出现了滑动窗口机制，它能够同时发送多个包，并且如果多个包都收到了，那么就直接发送最后一个包的 ACK，表示该包包括前面的所有包都收到了，这就是累计确认；
同时如果同时发送包1、包2、包3，其中包2丢失了，接收方处理好了 包1 和 包3，但是没有包 2，所以它会回应 ACK = 包2，表示没有收到包2，因此发送方会重发包2，接收方收到包2，处理完后，发现包3 也处理了，因此会发送 ACK = 包4
如果接收方收到重复的已处理的包，那么会发送 ACK 告知已经处理

    滑动窗口分为三个指针：字节流中已经发送的并且已经 ACK 的 offset 位置、已经发送但未 ACK 的 offset 位置，剩下的可发送的最后一个字节的 offset 位置
    TCP 发送缓冲区字节流：
    1	2	3	4	5	6	7	8	9	10	11	12	13	14
			👆				👆					👆
			i				j					k
	指针 i 表示该位置以及前面的数据已经发送并且已经收到 ACK，没有它们什么事了，可以直接舍弃
	指针 j 表示 [i + 1, j] 之间的数据是已经发送，但是未收到 ACK 的，需要保留，如果需要的话就进行重传
	指针 k 表示 [j + 1, k] 之间的数据是未发送，但是是可发送的，即表示接收方现在 buffer 缓冲区的可用大小，这些数据可以不用等待前面数据 ACK，可以直接发送
	指针 k 后面的数据表示未发送，并且也不能发送


4、流量控制（利用滑动窗口实现）
流量控制是针对一个 TCP 连接来说的，通过流量控制来限制发送方的发送数据量
如果没有对发送方的发送数据量进行限制，一股脑的有多少数据就发送多少数据，那么接收方处理不过来，会导致数据丢失
因此，发送方需要根据接收方的处理能力来发送数据，即接收方的 buffer 还能够存储多少数据。

滑动窗口的概念实际上是位于发送方的，表示已经发送了哪些数据，剩下的还可以再发送哪些数据，只不过窗口的大小是通过接收方来决定的

发送方会维护一个变量 receive window，简称 rwnd，它是用来记录接收方的 buffer 还可以存储多少数据
而这个数据实际上是接收方告知发送方的，在每次回复 ACK 的时候，接收方都会把 rwnd 的大小告知发送方，发送方通过这个变量来控制发送的数据量

在发送窗口为 0 的时候，接收方在处理完数据后不会主动告知 rwnd 大小，因此这时候 发送发会不断发送 1B 数据包给接收方，当接收方处理完数据，能够接收到这 1B 数据后，那么就会发送 ACK 同时告知 rwnd 的大小，让发送继续发送数据
	（这里为了避免网络中只有小数据包，即糊涂窗口综合征，所以会默认开启 Negle 算法，同时也会导致粘包、拆包问题）
    
   
5、拥塞控制(利用拥塞窗口实现)
拥塞控制是针对整个网络环境来说的，发送方不是单单维护一个滑动窗口，还需要维护一个拥塞窗口
不是接收方告知发送方 rwnd，那么发送方就直接发送 rwnd 的数据，而是需要根据整个网络环境来说，避免整个网络环境发生拥堵
所以需要通过拥塞窗口来控制发送的速率
也就是说，滑动窗口是接收方用来控制发送方的发送数据量，而拥塞窗口是用来控制发送方的发送速率的，一个是考虑接收方的接收能力，一个是考虑网络环境的拥堵情况


1）慢启动：刚建立 TCP 连接时，不会立马发送大数据，而是会一点一点提高数据量
指定拥塞窗口 cwnd = 1，表示只能发送 1MSS 的数据量
当收到 1 个 ACK 后，cwnd + 1
当收到 2 个 ACK 后，cwnd + 2
当收到 4 个 ACK 后，cwnd + 4
。。。
它发送的数据量是层指数增长的

2）拥塞避免：慢启动可以让发送速率很快到达一个理想的值，但不可能一直指数增长下去，所以设置了一个慢启动阈值 ssthresh
当 cwnd 到达这个阈值时，进入拥塞避免算法
假设 cwnd = 8，那么它只有收到 8 个 ACK 后，cwnd + 1
后续收到 9个 ACK 后，cwnd + 1
即开始线性增长

3、快重传和快恢复
在快重传和快恢复还没有出现时，一旦丢包，出现超时重传，TCP 就认为是网络拥堵，那么就会重新开始慢启动过程，这样的话，就会导致在没有网络拥堵的情况下发送速率也降低了
因此出现了快重传和快恢复

如果在超时时间内收到 3 个重复的 ACK 时，那么不需要等待超时重传，开启快重传算法
如果在超时时间内没有收到 3 个重复 ACK，那么重新进行慢启动算法
				（因为出现重复 3 个 ACK 时丢包的概率比乱序的概率要大得多）
快重传：
	1）重传丢失的数据包
    2）将 cwnd 缩减为一半，再将慢启动阈值设置为 cwnd，即 cwnd = cwnd / 2, ssthresh = cwnd
    3）进入快速恢复算法

快恢复：
    1）将 cwnd = cwnd + 3
    	（为什么 +3？因为它认为已经收到 3 个重复的数据包了，那么表示网络中已经减少了 3个包，可以再放三个包进去）
    2）往后收到重复的 ACK，那么 cwnd +1
    3）如果收到新的 ACK，表示接收方已经收到新的数据包了，将 cwnd 设置为 ssthresh，退出快速恢复算法，由于 cwnd == ssthresh，所以开始拥塞避免算法
```

<img src="https://img2018.cnblogs.com/blog/1629488/201906/1629488-20190622120313249-1589098511.png" style="zoom:60%;" />









## 6、TCP 综合问题

```java
1、SYN flood 攻击（SYN 泛洪攻击）
该攻击是针对 TCP 三次握手的
TCP 存在两个队列：SYN 队列（半连接队列）和 Accept 队列（全连接队列），它们的空间大小是有限的

两个队列的使用场景：
1）客户端发起 SYN 连接，服务器响应 SYN + ACK，然后将这个 TCP 连接存储到 SYN 队列中
2）客户端响应 ACK，服务器接收到后，将 TCP 连接从 SYN 队列中移除，存储到 Accept 队列，等待 accept() 调用

而 SYN flood 攻击就是黑客不断伪造 ip，发起 SYN 连接，致使服务器不断将 TCP 连接存储到 SYN 队列中，导致其他的用户的正常连接由于 SYN 队列满了而被舍弃，无法得到正常响应

解决方法：SYN cookie：
当 SYN 队列满了的时候，对于其他 SYN 连接不直接舍弃，而是通过 SYN cookie 的方式来解决：
    1）在服务器响应 SYN + ACK 的时候，需要初始化一个随机数作为初始序列号，而这里是将 客户端 ip、端口、服务器 ip、端口 以及其他的安全信息进行 hash 计算得到一个 hash 值，然后将这个 hash 值代替随机数作为初始序列号，这个 hash 值就叫做 SYN cookie
    2）客户端响应 ACK 时，服务器将 ACK 报文中的 ACK number - 1，然后将相同的信息经过相同的 hash 算法，将两个 hash 值进行比较，如果一致那么表示连接成功，将该 TCP 连接存储到 Accept 队列中
    3）如果短时间内收到重复 IP 的 SYN 连接，那么认定为攻击，以后这个 IP 地址的包都会被舍弃（不过真正的攻击都是变换 IP 的）

不直接舍弃 SYN 队列而全部采用 SYN cookie 的原因：
计算和验证 SYN cookie 需要 CPU 进行大量计算，如果在高并发的情况下，那么会占用很多的 CPU 资源

    当出现 SYN 攻击时，可以通过 netstat 命令的 netstat -anp | grep SYN_RECV 查询处于 SYN_RECV 状态的 TCP 连接
    


2、糊涂窗口综合征
如果接收方的 rwnd 一直都很小，那么发送方每次都只能发送小数据包，而一个 IP 报文 + TCP 报文 至少占用 40B，如果数据只有 1B 的话，那么这个数据包的性价比显然是很低的，占据网络带宽 41 B，而真正有意义的只有 1B，这种叫做糊涂窗口综合征

为了使得网络中存在太多这种小数据包占据网络带宽，TCP 默认开启了 Nagle 算法：
只有满足以下几种情况之一才会发送数据：
    1）发送缓冲区的数据达到 MSS
    2）发送缓冲区存在 FIN 标志位
    3）超过一定的时间


    
    
3、粘包、拆包
粘包表示属于不同包的数据合并在一个包一起发送，比如 A 数据 10 bytes, B 数据 20 bytes，两个数据在接收方的缓冲区中黏在一起了
拆包表示属于一个包的数据被拆成多个包，并且跟其他的包发生粘包

只有 TCP 才会发生粘包，UDP 不会发生粘包
因为 TCP 是字节流，数据之间没有明显的界限，所以接收方在处理数据的时候会很难办，因为不知道哪里是两个不同的数据

可能发生粘包、拆包的情况：
1）接收方没有及时处理数据，导致多个数据堆积
2）发送方开启了 Nagle 算法，导致多个不同的小数据黏在一起发送
3）应用层要发送的一个数据太大，超过 MSS，需要拆包发送，而拆分后后半部分数据跟别的数据黏在一起

粘包、拆包解决：
1）TCP 报文头部添加报文长度（这是需要修改协议了）
2）双方规定数据的固定长度
3）在数据的末尾添加一个结束标识符
4）关闭 Nagle 算法
5）约定先发个数据长度，数据长度添加一个结束标识符，防止粘包，然后根据长度进行数据读取（其实就是前面一种的结合）
    

这里讲个后话：很多人包括我上面都是说 粘包、拆包，实际上对于 TCP 协议来说没有什么包的概率，只有流的概念，它是将属于一个数据的字节数据称作一个包，这在 TCP 协议显然是错误的。
```



## 7、七层网络模型 / DNS、ARP、ping

```
1、七层网络模型：
1）应用层（HTTP、FTP、SMTP（简单邮件传输协议 Simple Mail Transfer Protocl）、DNS）
2）表示层：SSL/TCL
3）会话层
4）传输层：TCP、UDP
5）网络层：IP、ICMP（跟 TCP、UDP 同级，都是被 IP 封装，但是属于网络层协议）、ARP（工作在数据链路层，但是属于网络层协议）
6）数据链路层：MTU、VPN
7）物理层：


2、DNS 是什么？域名解析，将域名解析为 IP 地址
www.baidu.com：
1）com 是顶级域名，这种是固定的，比如 com 表示 company， 是商业用的
2）baidu 是权威域名，是用户自己注册的，比如百度创始人自己注册为 baidu
3）www 是主机名，在 baidu.com 下可以有很多主机名不同的子域名
实际上真正的域名应该是 www.baidu.com. ，即最后有一个根域名 '.'，不过所有的域名后面都有，所以直接省略了根域名

DNS 解析过程：
1）按照 浏览器缓存、操作系统缓存、hosts 文件的顺序进行访问，如果存在对应域名映射，那么直接使用（这也就是如果计算机的域名映射被黑客修改了，那么每次访问都会跳转到恶意网站上）
2）浏览器调用操作系统的一个函数，通过网卡给本地 DNS 服务器（我们网络配置的默认 DNS 服务器）发送一个 DNS 请求，本地 DNS 服务器收到后，查看自己的 DNS 缓存，如果存在的话那么直接返回。
3）本地 DNS 服务器内部会存储 根域名服务器（.），将 DNS 请求发送给 根域名服务器
	（这里讲下域名服务器的种类：根域名服务器（.）、顶级域名服务器（com）、权威域名服务器（baidu.com））
4）根域名服务器不会直接负责域名解析，它存储 顶级域名服务器的对应的 ip 地址，因此它会将 www.baidu.com 对应的顶级域名服务器（com）的 ip 地址告知本地 DNS 服务器
5）本地 DNS 服务器拿着这个 ip 地址去访问顶级域名服务器（com），顶级域名服务器维护顶级域名下的权威域名（baidu.com），因此它会将 www.baidu.com 中的权威域名服务器（baidu.com）的 ip 地址告知 本地 DNS 服务器
6）本地 DNS 服务器拿着这个 ip 地址去访问权威域名服务器（baidu.com），权威域名服务器维护权威域名下的子域名（www.baidu.com），因此它会将 www.baidu.com 的 ip 地址告知 本地 DNS 服务器。
7）本地 DNS 服务器将 ip 地址存储下来，然后返回给浏览器，浏览器收到后同时也会缓存下来，然后发起 TCP 连接请求

DNS 解析方式：递归 和 迭代

全世界存在 13组 根域名服务器，但是不是只有 13 台根域名服务器，因为它们存在从根域名服务器（镜像域名服务器），分担 DNS 查询请求

DNS 使用的传输层协议：
1）UDP：在默认情况下 DNS 使用 UDP 协议，因为不需要建立握手，也不需要保证什么可靠性，省去了 3 次 TCP 握手的时间，因为一次 DNS 解析需要多次请求，如果频繁的 TCP 连接效率非常低
2）TCP
	①、区域传送：DNS 服务器采用集群的方式，存在主从域名服务器，从域名服务器启动时需要跟主域名服务器同步数据，这时候就需要使用可靠的 TCP 来同步数据
	②、数据大于 512B：DNS 的 UDP 最多只能传输 512B 数据，一旦超过 512B，而 UDP 包又不能分割，所以只能使用 TCP。

DNS 劫持：攻击的方式之一是 黑客攻击了计算机的路由器，将本地 DNS 域名服务器设置为自己搭建的 DNS 域名服务器，无论什么域名都返回自己的恶意网站的 ip 地址，这样用户就会一直访问这个恶意网站
	

3、ARP
ARP 协议：使用 IP 地址到 MAC 地址的映射

这里讲下交换机和路由器：
同个网段的不同主机连接的是交换机（不一定是同一台，可能是不同的交换机，不过这些交换机之间互相连接），通过交换机来进行通信，而交换机是二层网络，只识别 MAC 地址。
由于交换机连接的是同个网段的主机，所以不同网段的主机间的通信不能只同个交换机，需要转发给网关进行路由，找到目的主机所在的网段，然后交给该网段的交换机，转发给目的主机。即交换机是用来通信，路由器是用来寻路的。
路由器（网关）本身存在多个接口，连接了多个网段，因此可以说路由器是多个网段的，可以跟不同网段的主机进行通信

我们需要记住，在路由转发的过程中，会改变的只有目的 MAC、源 MAC，而 源 IP 和 目的 IP 是不会发生改变的，比如主机 A 和 主机 B 路径上存在 3 个路由器，路由器1、路由器2、路由器3，那么在路由转发的过程中，路由器1 将消息路由转发给路由器2 时，数据链路层的消息如下：源 IP（主机 A）、目的 IP（主机 B）、源 MAC（路由器1）、目的 MAC（路由器2），后面的传输也是如此
反正就是根据 IP 地址来确定目的路径，而 MAC 是在路径上不断进行中转的
	（IP 比如我到医院，而 MAC 这段路径不断转换的路程，比如广州南->车陂南，车陂南->杨箕，杨箕->农讲所，最终步行到达目的地）
路由器进行下一跳的数据传输也是需要经过数据链路层到达下一个路由器的，所以需要 MAC 地址，而不是凭空发送。


同网段 ARP 解析过程：
    1）主机 A 将 源 IP（主机 A）、目的 IP（主机 B）、源 MAC（主机 A） 封装为一个 ARP 请求报文（目的 MAC 全是 0），调用数据链路层接口，交给数据链路层
    2）数据链路层将 ARP 请求报文封装为数据帧，MAC 报文头部中  源 MAC （主机 A），目的 MAC（全 F，表示广播地址），发送到交换机
    	（一个数据帧包含源 MAC 和 目的 MAC，内部的 ARP 报文也包含源 MAC 和 目的 MAC，
    			不过 ARP 报文的目的 MAC 是全 0，数据帧的目的 MAC 是全 F
    			相当于 ARP 报文实际上是替代了原来数据帧中 IP 报文的位置，所以才说 ARP 是网络层协议）
    3）交换机收到数据帧后，查找自己的 MAC 表，如果匹配了 MAC 表，那么根据对应的端口发送出去，如果没有匹配，那么进行广播（这里由于 目的 MAC 全是 F，是广播地址，所以会直接进行广播）
    4）其他主机收到 ARP 包后，如果上面的 IP 地址不是自己，那么舍弃；如果是自己，那么将主机 A 的 IP 和 MAC 进行映射缓存到 ARP 表中，再将 源 IP（主机 B）、目的 IP（主机 A）、源 MAC（主机 B）、目的 MAC（主机 A）封装为一个 ARP 响应报文，通过交换机单播给主机 A
    5）主机 A 收到后，将 MAC 地址缓存起来，完成 ARP 解析


跨网段 ARP 解析（ARP 代理）：
    由于路由器会隔绝两个不同网段之间的广播，所以如果主机 A 和 主机 B 不在同一个网段，那么主机 A 是收不到主机 B 的 ARP 响应报文的。
    1）所以主机 A 需要跟主机 B 通信，但是却不知道主机 B 的 MAC 地址时，主机 A 通过子网掩码发现主机 B IP 不在同个网段中，那么就需要通过网关（路由器）来发送，如果主机 A 只知道网关 A 的 IP 地址（我们网络配置的默认网关 IP），那么需要进行一次同网段的 ARP 解析（ARP 请求报文中目的 IP 为网关 A 的 IP），然后获取到网关 A 的 MAC 地址，主机 A 缓存网关 A 的 MAC 地址。
    2）此时的 ARP 解析实际上已经完成了。

这里讲下主机 A 缓存了网关 A 的 MAC 地址后，如何跟主机 B 进行通信：
    1）主机 A 发送给主机 B 的消息是通过交换机先发送网关 A 的，此时消息信息为 源 IP（主机 A）、目的 IP（主机 B）、源 MAC（主机 A）、目的 MAC（网关 A）
    2）网关 A 收到消息后，如果网关 A 和 主机 B 在同个网段（上面也说了路由器是多网段的），那么一次同网段 ARP 解析获取主机 B 的地址，然后网关 A 再将消息发送给数据链路层（交换机）发送给主机 B
    3）如果 网关 A 和 主机 B 不在同个网段，中间需要经过多个路由器，那么网关会根据 目的 IP 找到下一跳的网关 B 地址，如果不存在网关 B 的 MAC，那么进行一次 ARP 解析，获取网关 B 的 MAC 地址，然后交给数据链路层，数据链路层封装的数据帧头部为 源 MAC（网关 A）、目的 MAC（网关 B），然后通过交换机发送给网关 B。网关 B 收到后，按照同样的方法，直接路由转发到主机 B
    4）主机 B 收到消息后，如果要给主机 A 发送消息，那么也是按照这种方法往回发


ARP 欺骗攻击：
这种攻击都是存在 “中间人”
    1）假设网络中存在 3 台主机 P1、P2、P3， P1 要跟 P2 进行通信，但是不知道 MAC 地址，所以会发送 ARP 广播
    2）正常情况下 P3 收到 ARP 请求报文后是会直接舍弃的，但是它作为中间人，会将自己的 MAC 地址作为 P2 的 MAC 地址，封装成 ARP 响应报文单播给 P1
    3）P2 收到后发现是自己的 ip，同样进行单播给 P1
    4）此时 P1 就会收到 P2 和 P3 的 ARP 响应报文，它不知道选择哪一个，但是它有一个决策，相信后来的 ARP 响应报文
    5）正是由于这个决策，所以 P3 为了保证自己能够被 P1 选中，所以它会不断的发送 ARP 响应报文，以此来覆盖 P2 的 ARP 响应报文
    6）最终 P1 选择了 P3，那么 P1 给 P2 发送的消息都会发送到 P3，这样 P3 就持有通信数据了，如果 P2 要给 P1 发送消息，那么 P3 可以使用相同的方法


4、ping
ping 使用的是 ICMP 协议
ICMP 协议 和 ARP 协议都是网络层协议，DNS 协议是应用层协议
ICMP 报文跟 TCP、UDP 一样，都是被 IP 报文进行封装，不过所处的层不一样

但主机 A 对 主机 B 发起 ping www.baidu.com 时：
1）首先主机 A 创建一个 ICMP 报文，
2）然后使用 DNS 进行域名解析，获取 目的 ip 地址
3）将 ICMP 报文、源 IP 地址、目的 IP 地址 交给 IP 层协议（调用 IP 层协议对外提供的接口 api）
4）IP 层协议将 源 IP 地址、目的 IP 地址 和 其他信息（数据的协议、TTL、校验和 等）封装成 IP 头部，将 IP 头部 和 ICMP 报文封装成 IP 报文
5）如果不知道 目的 MAC 地址，那么需要经过 ARP 解析，根据 IP 地址获取 目的 MAC 地址，然后将 IP 报文、源 MAC 地址、目的 MAC 地址交给数据链路层
6）数据链路层将 源 MAC 地址、目的 MAC 地址封装成 MAC 头部，组成在 IP 报文上，封装成数据帧，然后进行转发
7）主机 B 收到数据帧后，先检验 MAC 地址是否一致，再检验 IP 地址是否一致，如果都一致，那么响应一个 ICMP 响应报文。
8）在一定时间内，主机 A 收到了 ICMP 响应报文，那么说明 主机 A 和 主机 B 是可达的
```

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTE5NTYwOS8yMDE3MTIvMTE5NTYwOS0yMDE3MTIyODIwMDQ1MTkxMy0xODMyNTcwMzUucG5n?x-oss-process=image/format,png" style="zoom:70%;" />



下面是 ARP 和 数据帧头部，即一个数据帧包含源 MAC 和 目的 MAC，内部的 ARP 报文也包含源 MAC 和 目的 MAC

不过 ARP 报文的目的 MAC 是全 0，数据帧的目的 MAC 是全 F

![img](https://pic1.zhimg.com/v2-96c46b4413e5a6c3d0a4324ddc92aeac_b.jpg)



## 8、浏览器访问 url 的整个过程

```
url 的大致过程：
1）输入 url
2）判断缓存
3）DNS 解析
4）ARP 解析
5）TCP 握手 / SSL 握手，发送 HTTP 请求
6）服务器响应请求
7）判断是否需要 cache
8）html 解析、渲染，如果是 HTTP 1.1，那么需要再发起请求获取 css、js、图片等资源，如果 HTTP 2.0，那么服务器会进行推送


1、浏览器输入 www.baidu.com

2、判断缓存
获取浏览器缓存，判断是否有 URL 请求的资源
如果存在，那么根据 expires 和 cache-control 判断缓存是否过期，如果没过期，那么继续使用，如果已经过期，那么需要向服务器发起Head 请求判断资源是否发生过修改。
如果不存在，那么需要向服务器发起 GET 请求
	以上不同情况服务器响应的状态码是不同的，如果是存在缓存但缓存过期了，而服务器资源没有修改过，那么服务器返回 304，如果没有缓存或者资源已经修改，那么服务器返回 200

3、DNS 解析
DNS 协议是应用层的

由于访问的是域名，那么需要进行进行域名解析，先按照浏览器缓存、操作系统缓存、hosts 文件的顺序进行访问，如果不存在，那么向本地 DNS 服务器发起 DNS 解析请求：
1）本地 DNS 服务器收到请求后，查看自己的缓存，如果有酒返回，没有那么向根域名服务器（.）发起 DNS 请求
2）根域名服务器不负责域名解析，它管理所有的顶级域名服务器，收到请求后，会将管理该域名的顶级域名服务器（.com）的 ip 地址发送给本地 DNS 服务器
3）本地 DNS 服务器拿到顶级域名服务器（.com）的 ip 地址，向它发起 DNS 请求
4）顶级域名服务器（.com）管理所有 .com 所有子域名的权威域名服务器，它会将管理该域名的权威域名服务器（baidu.com）发送给本地 DNS 服务器
5）本地 DNS 服务器拿到权威域名服务器（baidu.com）的 ip 地址，向它发起 DNS 请求
6）权威域名服务器（baidu.com）管理 baidu.com 所有子域名的 ip 地址，它会将 www.baidu.com 的 ip 地址返回给本地 DNS 服务器
7）本地 DNS 服务器将 ip 地址存储起来，同时返回给浏览器，完成 DNS 解析

DNS 解析涉及到递归和迭代两种方式

DNS 默认情况下是使用 UDP，因为如果使用 TCP 的话每次请求都需要三次握手 + 四次挥手，这样的话需要 3.5 个 RTT，物理距离越长耗费的时间越长，效率低。
不过 DNS 的 UDP 限制最大为 512B，如果超过 512B 就不能使用 UDP
在区域传送以及 DNS 报文长度超过 512B 的时候使用 TCP（DNS 好像为了防止伪造 DNS 域名服务器的应答，跟 HTTPS 一样有个证书链，加上证书链的 DNS 超过几千字节）

4、ARP 解析
ARP 协议是网络层的
如果 DNS 解析获取 IP 地址后，没有对应的 MAC 地址，那么需要进行 ARP 解析

5、TCP 三次握手 / SSL 四次握手
在没有指定 http/https 协议的情况下，浏览器默认会访问 http 协议
www.baidu.com 如果是 https 协议，服务器会同时监听 https 和 http 的 80 端口
访问过程如下：
1）以 HTTP 协议的方式建立 TCP 三次握手，然后发送 HTTP 请求
2）服务器收到后，响应 302 状态码（临时重定向），让浏览器以 https 的方式访问
3）浏览器以 HTTPS 协议的方式建立 TCP 三次握手 + SSL 四次挥手，发送 HTTP 请求
    

6、服务器响应请求，返回 HTML 资源


7、判断缓存
浏览器接收服务器响应，如果是 2 中存在缓存但是缓存过期了而发起的请求，那么如果资源未改变，服务器响应 304，那么浏览器继续使用原来的缓存
如果是不存在缓存而发起的请求，那么这里判断是否能够进行缓存，如果可以那么进行缓存。

8、html 解析、渲染
进行 html 的解析，在 html 中可能会需要一些 js、css、图片等
如果是 HTTP 1.1，那么需要将重新发起多个请求来一一获取这些资源
如果是 HTTP 2.0，如果开启了服务推送功能（服务器推送是 HTTP 2.0 唯一一个需要手动配置的），那么服务器会进行推送，无需再次发起请求
```





## 9、cookie、session、token

```java
HTTP 是无状态的，在单单 HTTP 协议中，服务器不会去记录上一次请求的是谁，以及谁是否已经请求过了
但是随着网络技术的发展，不想改动 HTTP 协议又想保持状态，比如登录状态，不想每次访问都去登录，因此需要另外的一种机制来跟 HTTP 协议配合使用
cookie 机制 和 session 机制就是为此诞生

1、cookie 机制：
cookie 和 session 不是同时出现的， cookie 要比 session 更早出现
ccokie 机制要求浏览器来存储数据，比如登录，用户在第一次访问的时候，请求发送到服务器，服务器发现用户没有登录，那么会让用户跳转到登录页面，在用户登录完成后，服务器会创建一个 cookie，然后将用户的信息设置到 cookie 中，这样用户后续请求的时候只要携带上这个 cookie，服务器通过 cookie 可以得知用户已经登录，那么就不会再让用户跳转到登录页面。

cookie 是一段文本，存储的是 key-value 的键值对，比如 name = "张三" 之类的
它的默认大小为 4KB，即存储的数据是有限的

cookie 的类型：
1)如果没有设置过期时间，那么 cookie 是存储在内存中的，它的生命周期只在浏览器打开的期间，一旦关闭浏览器，那么 cookie 就会失效，这种 cookie 叫做 “session cookie”（会话 cookie）
2)如果设置了过期时间，那么 cookie 会被持久化到磁盘上，这样的话在过期时间内，即使浏览器关闭了，那么 cookie 还是存在的，这种 cookie 叫做 "persistent cookie"（持久化 cookie）

cookie 的缺点：
1）cookie 的存储大小和数据类型是有限的，一个 cookie 默认是 4KB，并且只能存储 key-value 形式的文本
2）cookie 存储在浏览器，容易被人 篡改 和 盗用，不安全，比如容易发生 CSRF 攻击


2、session 机制
为了解决 cookie 的不安全问题，出现了 session 机制
session 机制是使用一个叫 session 的数据结构来存储用户的信息的
当用户第一次访问时，服务器会为该用户创建一个 session，然后将 session 存储在服务器的内存上（可以持久化到文件、缓存中）
使用这种机制的话，那么就需要一种机制来标识哪个 session 属于哪个用户，因此每个 session 会存在一个 JSESSIONID，可以认为是 key-value 的映射关系。

而 session 机制一般情况下需要借助 cookie 机制来实现，服务器保存 session 和 JSESSIONID，浏览器通过 cookie 保存 JSESSIONID，在后续访问的时候，只需要携带上 cookie 然后服务器根据 cookie 中的 JSESSIONID 获取对应的 session 即可。
用户信息是存储在服务器的，不用担心被盗用和篡改。

session 机制的缺点：
1）session 存储在服务器内存，容易造成服务器的压力
2）在集群环境下，session 只会存在于第一次用户访问的服务器，如果下一次用户访问的是别的服务器，那么会导致用户的状态丢失（这里可以使用 粘性 session、session 复制、redis 存储 session 来解决）

cookie 和 session 的区别：
1）cookie 存储在客户端，相对来说不安全，容易被黑客攻击，session 存储在服务器，比较安全
2）cookie 的容量和存储的数据类型是有限的，session 则没有这方面的限制

如果 cookie 被禁用了，有两种方法：
1）让 浏览器只保存 JSESSIONID，然后在请求的时候将 JSESSIONID 放到 URL 上
2）使用 token 来代替 session（JWT）


3、JWT：
JWT 分为三部分：header、payload（载荷）、签名

header 存储 token 类型（比如 JWT）和 生成签名时的散列算法
payload 存储 token 的信息，比如过期时间、颁发的服务器、颁发给的用户
使用 base64 将 header 和 payload 进行编码（注意不是散列算法），方便传输
然后使用 "." 串起来，比如
eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0

然后再选定一个密钥 secret，使用 header 头部指定的散列算法对这串字符串进行 hash 计算，得到签名/信息摘要，再串到上面的字符串上去，变成：
eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM

可以看出，header 和 payload 是明文的，因此 JWT 不能用来存储敏感信息，并且一个 JWT 具有一次性，不能直接实现续签，因为一旦修改了一点信息，那么它的签名都会发生改变

在验证的时候，使用 header、payload、secret 再经过相同的计算得到签名，跟 JWT 上面的签名进行比较即可（跟 CA 证书的验证差不多）
```





## 10、TCP 和 UDP 的区别

```java
1、TCP 是面向连接的，在第一次发送数据前需要经过 3 次握手，在断开连接前需要经过 4 次挥手；UDP 无连接，什么时候想发送数据直接发送即可
2、TCP 基于字节流，数据之间不存在界限，由于在 TCP 头部不存在字段标识报文长度，所以需要在应用层进行处理，解决粘包；UDP 是基于报文的，在 UDP 头部存在字段能够标识报文长度，所以不会发生粘包（实际上 TCP 协议本身没有包的概念，感觉这个粘包的描述有点问题）
3、TCP 可以进行分片，因为它是字节流，没有数据界限，当数据超过 MSS 时可以在 IP 层进行分片；UDP 不能分片，当 IP 报文长度超过 MTU，即 UDP 报文长度超过 MSS，那么 UDP 包将被舍弃
4、TCP 由于是连接，所以是点对点的；UDP 本身不存在连接的概念，那么可以是一对一、一对多
5、TCP 通过序列号和确认号机制、超时重传机制、滑动窗口机制、流量控制机制、拥塞控制机制来保证数据的可靠性传输；UDP 本身并不存在这些机制，它不负责可靠性，因此发生丢包问题也不会进行处理

简单讲就是 TCP 由于需要实现了可靠性传输，所以效率会比 UDP 差很多，同时它的拥塞控制机制是在操作系统内核实现的，无法在应用层进行修改，所以在很多情况下都显得太重，因此在 HTTP 3.0 中才会使用 UDP 来实现。
```





## 11、子网划分、子网掩码、网络段、主机号、私网、公网

```
所有的 IP 地址为 [00000000 00000000 00000000 00000000] - [11111111 11111111 11111111 11111111]
				0.0.0.0 - 255.255.255.255

IP 地址分为 A、B、C、D、E 类地址，
A 类地址中，8bit 为网络号，24bit 为主机号，子网掩码为 255.0.0.0
B 类地址中，16bit 为网络号，16bit 为主机号，子网掩码为 255.255.0.0
C 类地址中，24bit 为网络号，8bit 为主机号，子网掩码为 255.255.255.0

子网掩码的作用：
每个网络号称之为网段，通过 子网掩码 & IP 地址 来将 IP 地址中的主机号置 0，忽略掉两个 IP 地址的主机号差异，保留网络号，以此来知道 源 IP 地址 和 目的 IP 地址是否处于同一个网段

C 类地址，一个网段可以同时存在 2^8 - 2 = 254 个主机
	（主机号范围为 00000000 - 11111111， 共 256 ，除去 全 0 和 全 1 有特殊作用，一个网段可以存在 254 个主机号）
在同个网段中的主机，可以直接通信，不需要经过网关
	比如 192.168.1.2 和 192.168.1.5 可以直接通信，它们在同个网段 192.168.1.0 中，主机号分别为 2 和 5
不同的网段主机不能直接通信，需要经过网关
	比如 192.168.1.2 和 192.168.2.5 不能直接通信，它们在不同网段 192.168.1.0 和 192.168.2.0 中
网关是用来连接两个不同网段的设备，比如 192.168.1.0 和 192.168.2.0 之间可以通过网关进行连接

什么是子网划分？
默认情况下分配的是一个网段，比如 C 类网段 192.168.1.0，它含有 254 个主机号，同时也是一个子网。
而更细的子网划分，这个 C 类网段可以再划分出子网，比如划分为两个大小相同的子网，子网掩码为 255.255.255.128，这样 子网 1 的主机号范围为 
[192.168.1.1 - 192.168.1.127]，子网 2 的主机号范围为 [192.168.1.128 - 192.168.1.254]

子网划分的好处：
1）减少广播的范围：一个子网可以认为就是一个网段，通过网关隔绝不同的网段，使得广播包只能在某个网段中传输，网络规模小了，占用的资源也少。
	这就是通过网关来隔绝广播域
2）节省 IP 资源：比如一个公司作为一个子网通信，内部只有 20 台计算机，那么如果直接分配一个 C 类网段 192.168.1.0，那么它只会占用这个 C 类地址的 20 个 IP，剩下的会浪费了，所以它可以对这个 C 类网段再进行划分，划分为多个子网，然后分其中一个子网给这个公司，剩下的子网分给别的公司


公网、私网：
首先我们先认为所有的 IP 地址都是公网，公网是能够在全球进行通信的，它就是上面划分的 A、B、C 类地址
而有的时候，一些计算机并不需要更全世界进行通信，只需要在我们这个局域网中进行通信，这时候为了节省 IP ，不可能每台主机都分配一个公网 IP，因此，它将 A、B、C 类地址中一部分地址划分出来，从公网 IP 变成 私网 IP，只要看到这些 IP，就知道是私网 IP：
A 类私网 IP：10.0.0.0-10.255.255.255
B 类私网 IP：172.16.0.0-172.31.255.255
C 类私网 IP：192.168.0.0-192.168.255.255
这些 IP 只能用于局域网通信，不能在全球通信，而这些私网 IP 在一个局域网中是不能重复的，否则一个 IP 指向多个主机，这就会产生混乱。而在不同的局域网中是可以重复的，反正也不会使用这个互相通信，重复了也没问题，因此可以进行复用。
而这些私网 IP 的计算机要跟外部进行通信，那么就需要转换为 公网 IP，由于 公网 IP 有限，所以多个私网 IP 共用一个公网 IP，通过 NAT 技术来实现。
```

